{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b434cbc",
   "metadata": {},
   "source": [
    "# Uses Frontier OpenAI to transcribe audio to text and then summarize that audio for users to review using Hugging Face Open Source Models on google colab.\n",
    "# I.E Speach-To-Text Summarization\n",
    "\n",
    "Utilizes:\n",
    "- Hugging Face Open Source Models and pipelines\n",
    "- OpenAi Speech to text transcription\n",
    "- Tokenizer\n",
    "- Quantization\n",
    "- Gradio Audio Interface with File upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0959583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install\n",
    "!pip install -q --upgrade bitsandbytes accelerate transformers==4.57.6\n",
    "!pip install -q gradio torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99af4e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize api\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "if hf_token:\n",
    "    print(f\"HF Token exists\")\n",
    "else:\n",
    "    print(\"HF Token not set\")\n",
    "\n",
    "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b037f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sign in to HuggingFace Hub\n",
    "\n",
    "login(hf_token, add_to_git_credential=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e060e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99edb23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt fucntion\n",
    "\n",
    "def get_prompts(transcription):\n",
    "    system_prompt = \"\"\"\n",
    "    You produce minutes of meetings from transcripts, with summary, key discussion points,\n",
    "    takeaways and action items with owners, in markdown format without code blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Below is an extract transcript of a Denver council meeting.\n",
    "    Please write minutes in markdown without code blocks, including:\n",
    "    - a summary with attendees, location and date\n",
    "    - discussion points\n",
    "    - takeaways\n",
    "    - action items with owners\n",
    "\n",
    "    Transcription:\n",
    "    {transcription}\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "              {\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe function\n",
    "\n",
    "def transcribe_audio(audio_file_path):\n",
    "\n",
    "  #Use of Hugging Face PipeLine to transcribe using openAI opensource model\n",
    "#   # Define and call pipeline\n",
    "#   pipe = pipeline(\n",
    "#     \"automatic-speech-recognition\",\n",
    "#     model=\"openai/whisper-medium.en\",\n",
    "#     dtype=torch.float16,\n",
    "#     device='cuda',\n",
    "#     return_timestamps=True\n",
    "# )\n",
    "\n",
    "#   result = pipe(audio_input)\n",
    "#   open_source_transcription = result[\"text\"]\n",
    "\n",
    "  # Open File\n",
    "  audio_file= open(audio_file_path, \"rb\")\n",
    "\n",
    "  # Frontier model transcription\n",
    "  AUDIO_MODEL = \"gpt-4o-mini-transcribe\"\n",
    "\n",
    "  frontier_transcription = openai.audio.transcriptions.create(model=AUDIO_MODEL, file=audio_file, response_format=\"text\")\n",
    "\n",
    "  # Transcribe Text \n",
    "  tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  inputs = tokenizer.apply_chat_templates(get_prompts(frontier_transcription), return_tensors=\"pt\").to(\"cuda\")\n",
    "  streamer = TextStreamer(tokenizer)\n",
    "  model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
    "  outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
    "  response = tokenizer.decode(outputs[0])\n",
    "\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb99aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio UI\n",
    "\n",
    "message_output = gr.Markdown(label=\"Summary\")\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=transcribe_audio,\n",
    "    inputs=gr.Audio(type=\"filepath\"),\n",
    "    outputs=[message_output],\n",
    "    flagging_mode=\"never\",\n",
    ")\n",
    "\n",
    "view.launch(share=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
