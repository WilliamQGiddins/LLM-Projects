{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyZHk8aufQCM"
      },
      "source": [
        "# Synthetic Data Generator using various Hugging Face open-source models and  frontier models on google colab to generate a business dataset. Users can optionally provide a sample schema.\n",
        "\n",
        "Utilizes \n",
        "- Quantization to shrink model size and lower gpu memory usage\n",
        "- Hugging Face Open Source models called with the tokenizer and parsed to only return the assistant response\n",
        "- JSONL parsing\n",
        "- Use of the Panda library for data analysis manipulation and grid preview\n",
        "- File generation for multiple types\n",
        "- Error handling\n",
        "- Gradio Blocks with multiple inferface objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_iNekE8fjaN",
        "outputId": "f869d9e2-56ca-4c6f-e50a-ffeab38eae33"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q --upgrade bitsandbytes anthropic accelerate transformers==4.57.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e8jnqGagAGB"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "import json\n",
        "import anthropic\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "import gc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZrcKGpxshE0"
      },
      "outputs": [],
      "source": [
        "# Define API Keys\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "#anthropic_api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "#google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Sign in to HuggingFace Hub\n",
        "login(hf_token, add_to_git_credential=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FogG6cmdu60K"
      },
      "outputs": [],
      "source": [
        "# Quantization\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez2i5jHRgIjI"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "LLAMA = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "GEMMA2 = \"google/gemma-2-2b-it\"\n",
        "GPT = \"gpt-4o-mini\"\n",
        "CLAUDE = \"claude-3-haiku-20240307\"\n",
        "GEMINI = \"gemini-2.0-flash\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlS1wUGlpEkR"
      },
      "outputs": [],
      "source": [
        "# Models\n",
        "\n",
        "MODELS = {\n",
        "    'LLama 3.2' : LLAMA,\n",
        "    'Phi 3 mini': PHI3,\n",
        "    'Gemma 2': GEMMA2,\n",
        "    'GPT 4.o mini': GPT,\n",
        "    'Claude 3 Haiku': CLAUDE,\n",
        "    'Gemini 2.0 Flash': GEMINI,\n",
        "}\n",
        "\n",
        "HF_MODELS = [LLAMA, PHI3, GEMMA2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrrmhv1f1GIN"
      },
      "outputs": [],
      "source": [
        "# File Formats\n",
        "\n",
        "FILE_FORMATS = [\".csv\", \".tsv\", \".jsonl\", \".json\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlOdddli7hq-"
      },
      "outputs": [],
      "source": [
        "SCHEMA = [\n",
        "    (\"Name\", \"TEXT\", \"Name of Toy\", \"He-Man Fused with Skeletor\"),\n",
        "    (\"Toy_Store\", \"TEXT\", \"Name of Toy Store\", \"Victory Toy Store\"),\n",
        "    (\"Address\", \"TEXT\", \"Toy Store address\", \"432 Brook Ave, Brooklyn, NY 11345\"),\n",
        "    (\"Type\", \"TEXT\", \"Toy Type\", 'One of [\"Action Figure\",\"Doll\",\"Video game\",\" Gaming Console\",\"Learning\"] or other potential types'),\n",
        "    (\"Price\", \"TEXT\", \"Toy Price\", \"$45, or '--' if unknown\"),\n",
        "    (\"Year\", \"INT\", \"Year toy distributed\", 2015),\n",
        "    (\"Units_Sold\", \"INT\", \"Number of Units Solde\", 300),\n",
        "    (\"Locations\", \"Array\", \"Other Locations Sold at\", '[\"123 Haven Rd, Newark, NJ 28563\", \"321 Scottsdale Drive, Orlando, FL 51943, ...]'),\n",
        "]\n",
        "\n",
        "DEFAULT_SCHEMA_TEXT = \"\\n\".join([f\"{i+1}. {col[0]} ({col[1]}) - {col[2]}, example: {col[3]}\" for i, col in enumerate(SCHEMA)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB_jF-rWgTKq"
      },
      "outputs": [],
      "source": [
        "# Prompts\n",
        "\n",
        "system_prompt = \"\"\"\n",
        "You are a expert assistant in generating synthetic datasets for businesses tailored to a given business case and user requirements.\n",
        "You accept a business case of the data and the use case for the dataset.\n",
        "If the user does not specify the output columns, infer and create the most appropriate columns based on your expertise.\n",
        "Do not repeat column names or column values across rows and only out valid JSONL.\n",
        "\"\"\"\n",
        "\n",
        "def get_user_prompt(business_case, num_records, schema):\n",
        "  user_prompt = f\"Create a sample dataset for my business case described here: {business_case}. Generate {num_records} for the dataset should in valid JSONL format based on the business case.\"\n",
        "\n",
        "  if schema is not None:\n",
        "    user_prompt += f\"Each line should be a JSON object with the following fields: \\n{schema}\\n\"\n",
        "\n",
        "  return user_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic3ljN_dq7JU"
      },
      "source": [
        "# Define LLM functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O8VcjMkzq2yD"
      },
      "outputs": [],
      "source": [
        "# GPT\n",
        "\n",
        "def call_gpt(model, user_prompt):\n",
        "  openai = OpenAI(api_key=openai_api_key)\n",
        "  messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt }]\n",
        "  response= openai.chat.completions.create(\n",
        "      model=model,\n",
        "      messages=messages,\n",
        "      temperature=0.7,\n",
        "    )\n",
        "\n",
        "  content = response.choices[0].message.content\n",
        "  return content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "A2aoCDa5sxjd"
      },
      "outputs": [],
      "source": [
        "# Claude\n",
        "\n",
        "def call_claude(model, user_prompt):\n",
        "  claude = anthropic.Anthropic(api_key=anthropic_api_key)\n",
        "  response = claude.messages.create(\n",
        "      model = model,\n",
        "      messages = [{\"role\": \"user\", \"content\" : user_prompt}],\n",
        "      temperature = 0.7,\n",
        "      max_tokens = 3000,\n",
        "      system = system_prompt\n",
        "    )\n",
        "\n",
        "  content = response.content[0].text\n",
        "  return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5QZCZWEYuJKW"
      },
      "outputs": [],
      "source": [
        "# Gemini\n",
        "\n",
        "def call_gemini(model, user_prompt):\n",
        "  gemini = OpenAI(api_key=google_api_key, base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
        "  messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt }]\n",
        "  response = gemini.chat.completions.create(\n",
        "      model=model,\n",
        "      messages=messages,\n",
        "      temperature=0.7,\n",
        "    )\n",
        "\n",
        "  content = response.choices[0].message.content\n",
        "  return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wD29AJjOulCy"
      },
      "outputs": [],
      "source": [
        "# Hugging Face\n",
        "\n",
        "def call_hf(model, user_prompt):\n",
        "  messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt }]\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
        "  model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=quant_config)\n",
        "  outputs = model.generate(inputs, max_new_tokens=2000)\n",
        "\n",
        "  _, _, after = tokenizer.decode(outputs[0]).partition(\"assistant<|end_header_id|>\")\n",
        "  content = after.strip()\n",
        "  return content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "pjtXFW9Cqql0"
      },
      "outputs": [],
      "source": [
        "# Choose LLM\n",
        "\n",
        "def choose_LLM(model_name, user_prompt):\n",
        "  try:\n",
        "    model = MODELS[model_name]\n",
        "\n",
        "    if \"gpt\" in model.lower():\n",
        "      response = call_gpt(model, user_prompt)\n",
        "    elif \"claude\" in model.lower():\n",
        "      response = call_claude(model, user_prompt)\n",
        "    elif \"gemini\" in model.lower():\n",
        "      response = call_gemini(model, user_prompt)\n",
        "    elif model in HF_MODELS:\n",
        "      response = call_hf(model, user_prompt)\n",
        "    else:\n",
        "      raise ValueError(f\"Model {model} not supported\")\n",
        "\n",
        "    lines = [line.strip() for line in response.strip().splitlines() if line.strip().startswith(\"{\")]\n",
        "\n",
        "    return [json.loads(line) for line in lines]\n",
        "\n",
        "  except Exception as e:\n",
        "    raise Exception(f\"Model query failed: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Viqa7HElx3zd"
      },
      "source": [
        "# Save File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2albbodVyAT6"
      },
      "outputs": [],
      "source": [
        "def save_dataset(file_name, records, file_format ):\n",
        "  df = pd.DataFrame(records)\n",
        "  print(df.shape)\n",
        "  if file_format == \"csv\":\n",
        "    df.to_csv(file_name, index=False)\n",
        "  elif file_format == \".tsv\":\n",
        "    df.to_csv(file_name, sep='\\t', index=False)\n",
        "  elif file_format == \".jsonl\":\n",
        "    with open(file_name, 'w') as f:\n",
        "      for record in records:\n",
        "        f.write(json.dumps(record) + '\\n')\n",
        "  elif file_format == \"json\":\n",
        "    df.to_json(file_name, orient='records', index=False)\n",
        "  else:\n",
        "    raise ValueError(f\"File format {file_format} not supported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93Z0mALxxcF9"
      },
      "source": [
        "# Generate Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "J8DQijBLiiRW"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(\n",
        "  model_name,\n",
        "  business_case,\n",
        "  num_records = 100,\n",
        "  schema = None,\n",
        "  file_format = '.jsonl',\n",
        "  file_name = 'sample_dataset.jsonl'\n",
        "):\n",
        "  \"\"\"\n",
        "  Generates a synthetic dataset using an LLM based on the given business case and optional schema.\n",
        "\n",
        "  Returns:\n",
        "      Tuple[str, pd.DataFrame | None]: A status message and a preview DataFrame (first 10 rows) if successful.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Validate record count\n",
        "    if num_records <= 10:\n",
        "      return \"‚ùå Error: Number of records must be greater than 10.\", None\n",
        "    if num_records > 1000:\n",
        "      return \"‚ùå Error: Number of records must be less than or equal to 1000.\", None\n",
        "\n",
        "    # Validate file format\n",
        "    if file_format not in FILE_FORMATS:\n",
        "      return f\"‚ùå Error: Invalid file format '{file_format}'. Supported formats: {FILE_FORMATS}\", None\n",
        "\n",
        "    # Validate file extension\n",
        "    if not file_name.endswith(file_format):\n",
        "      file_name += file_format\n",
        "\n",
        "    # Create prompt and call the appropriate model\n",
        "    user_prompt = get_user_prompt(business_case, num_records, schema)\n",
        "    records = choose_LLM(model_name, user_prompt)\n",
        "\n",
        "    if not records:\n",
        "      return \"‚ùå Error: No valid records were generated by the model.\", None\n",
        "\n",
        "    # Save Dataset\n",
        "    save_dataset(file_name, records, file_format)\n",
        "\n",
        "    # Prepare preview\n",
        "    df = pd.DataFrame(records)\n",
        "    preview = df.head(10)\n",
        "\n",
        "    success_message = (\n",
        "        f\"‚úÖ Generated {len(records)} records successfully!\\n\"\n",
        "        f\"üìÅ Saved to: {file_name}\\n\"\n",
        "    )\n",
        "\n",
        "    return success_message, preview\n",
        "\n",
        "  except Exception as e:\n",
        "    return f\"‚ùå Error: {str(e)}\", None\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87gbZfhH1-4u"
      },
      "source": [
        "# Generate Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djk2ZuwD2DWg"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks(title=\"Synthetic Data Generator\") as interface:\n",
        "  gr.Markdown(\"# Dataset Generator\")\n",
        "  gr.Markdown(\"Generate synthetic datasets using AI models\")\n",
        "\n",
        "  with gr.Row():\n",
        "    with gr.Column(scale=2):\n",
        "      schema_input = gr.Textbox(\n",
        "          label=\"Schema\",\n",
        "          value=DEFAULT_SCHEMA_TEXT,\n",
        "          lines=15,\n",
        "          placeholder=\"Define your dataset schema here... Please follow this format: Name (TYPE) - Description, example: Example\"\n",
        "      )\n",
        "\n",
        "      business_case_input = gr.Textbox(\n",
        "          label=\"Business Case\",\n",
        "          value=\"I want to generate a toy store dataset for inventory\",\n",
        "          lines = 2,\n",
        "          placeholder=\"Describe your business case here...\"\n",
        "      )\n",
        "\n",
        "      with gr.Row():\n",
        "        model_dropdown = gr.Dropdown(\n",
        "            label=\"Model\",\n",
        "            choices=list(MODELS.keys()),\n",
        "            value=list(MODELS.keys())[0],\n",
        "            interactive=True\n",
        "        )\n",
        "\n",
        "        num_records_input = gr.Number(\n",
        "            label=\"Number of Records\",\n",
        "            value=25,\n",
        "            minimum=11,\n",
        "            maximum=1000,\n",
        "            step=1\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "          file_name_input = gr.Textbox(\n",
        "              label=\"Save as\",\n",
        "              value=\"toystore_dataset\",\n",
        "              lines=1,\n",
        "              placeholder=\"Name your dataset file here...\"\n",
        "          )\n",
        "\n",
        "          file_format_dropdown = gr.Dropdown(\n",
        "              label=\"File Format\",\n",
        "              choices=FILE_FORMATS,\n",
        "              value=FILE_FORMATS[0],\n",
        "              interactive=True\n",
        "          )\n",
        "\n",
        "        generate_btn = gr.Button(\"üöÄ Generate\", variant=\"secondary\", size=\"lg\")\n",
        "\n",
        "    with gr.Column(scale=1):\n",
        "      gr.Markdown(\"\"\"\n",
        "      ### üìù Dataset Generation Instructions\n",
        "\n",
        "      1. **üóÇ Schema** ‚Äì Define your dataset structure\n",
        "        *(default: restaurant schema provided)*\n",
        "      2. **üí° Business Case** ‚Äì Enter a prompt to guide the AI for generating data\n",
        "      3. **ü§ñ Model** ‚Äì Choose your AI model: GPT, Claude, Gemini, or Hugging Face\n",
        "      4. **üìä Number of Records** ‚Äì Specify entries to generate\n",
        "        *(min: 11, max: 1000)*\n",
        "      5. **üìÅ File Format** ‚Äì Select output type: `.csv`, `.tsv`, `.jsonl`, or `.json`\n",
        "      6. **üíæ Save As** ‚Äì Provide a filename *(extension auto-added)*\n",
        "      7. **üöÄ Generate** ‚Äì Click **Generate** to create your dataset\n",
        "\n",
        "      ### üîß Requirements\n",
        "\n",
        "      Set API keys in Colab‚Äôs secret section:\n",
        "      `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, `GOOGLE_API_KEY`, `HF_TOKEN`\n",
        "      \"\"\")\n",
        "      output_status = gr.Textbox(\n",
        "        label=\"Status\",\n",
        "        lines=4,\n",
        "        interactive=False\n",
        "      )\n",
        "\n",
        "      output_preview = gr.Dataframe(\n",
        "        label=\"Preview (first 10 rows)\",\n",
        "        interactive=False,\n",
        "        wrap=True\n",
        "      )\n",
        "\n",
        "  generate_btn.click(\n",
        "      fn=generate_dataset,\n",
        "      inputs=[\n",
        "        model_dropdown,\n",
        "        business_case_input,\n",
        "        num_records_input,\n",
        "        schema_input,\n",
        "        file_format_dropdown,\n",
        "        file_name_input\n",
        "      ],\n",
        "      outputs=[output_status, output_preview]\n",
        "  )\n",
        "\n",
        "interface.launch(debug=True)\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
